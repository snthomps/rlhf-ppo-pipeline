import{r as e,a as s,g as n}from"./react-vendor-Z2Iecplj.js";import{r as t,Z as r,S as a,P as l,a as i,R as o,A as d,b as c,c as m}from"./lucide-Ds36g6rr.js";!function(){const e=document.createElement("link").relList;if(!(e&&e.supports&&e.supports("modulepreload"))){for(const e of document.querySelectorAll('link[rel="modulepreload"]'))s(e);new MutationObserver(e=>{for(const n of e)if("childList"===n.type)for(const e of n.addedNodes)"LINK"===e.tagName&&"modulepreload"===e.rel&&s(e)}).observe(document,{childList:!0,subtree:!0})}function s(e){if(e.ep)return;e.ep=!0;const s=function(e){const s={};return e.integrity&&(s.integrity=e.integrity),e.referrerPolicy&&(s.referrerPolicy=e.referrerPolicy),"use-credentials"===e.crossOrigin?s.credentials="include":"anonymous"===e.crossOrigin?s.credentials="omit":s.credentials="same-origin",s}(e);fetch(e.href,s)}}();var p,u,g={exports:{}},x={};var h,f=(u||(u=1,g.exports=function(){if(p)return x;p=1;var s=e(),n=Symbol.for("react.element"),t=Symbol.for("react.fragment"),r=Object.prototype.hasOwnProperty,a=s.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,l={key:!0,ref:!0,__self:!0,__source:!0};function i(e,s,t){var i,o={},d=null,c=null;for(i in void 0!==t&&(d=""+t),void 0!==s.key&&(d=""+s.key),void 0!==s.ref&&(c=s.ref),s)r.call(s,i)&&!l.hasOwnProperty(i)&&(o[i]=s[i]);if(e&&e.defaultProps)for(i in s=e.defaultProps)void 0===o[i]&&(o[i]=s[i]);return{$$typeof:n,type:e,key:d,ref:c,props:o,_owner:a.current}}return x.Fragment=t,x.jsx=i,x.jsxs=i,x}()),g.exports),v={};var b=function(){if(h)return v;h=1;var e=s();return v.createRoot=e.createRoot,v.hydrateRoot=e.hydrateRoot,v}();const y=()=>{const[e,s]=t.useState(!1),[n,m]=t.useState(0),[p,u]=t.useState("none"),[g,x]=t.useState(null),[h,v]=t.useState([]),b=t.useRef(null),y=(e,s="info")=>{v(n=>[...n,{message:e,type:s,time:(new Date).toLocaleTimeString()}])},_=()=>{b.current&&(clearInterval(b.current),b.current=null),s(!1),y("Training stopped by user","warning")},j=({level:s,title:n,speedup:t,features:r})=>f.jsxs("div",{onClick:()=>!e&&u(s),className:`p-4 border-2 rounded-lg cursor-pointer transition-all ${p===s?"border-blue-500 bg-blue-50":"border-gray-300 hover:border-gray-400"} ${e?"opacity-50 cursor-not-allowed":""}`,children:[f.jsxs("div",{className:"flex items-center justify-between mb-2",children:[f.jsx("h3",{className:"font-bold text-lg",children:n}),f.jsxs("span",{className:"px-2 py-1 rounded text-sm font-bold "+(t>3?"bg-green-200 text-green-800":t>2?"bg-blue-200 text-blue-800":t>1?"bg-yellow-200 text-yellow-800":"bg-gray-200 text-gray-800"),children:[t,"x"]})]}),f.jsx("ul",{className:"text-sm space-y-1",children:r.map((e,s)=>f.jsxs("li",{className:"flex items-start",children:[f.jsx("span",{className:"mr-2",children:"â€¢"}),f.jsx("span",{children:e})]},s))})]});return f.jsx("div",{className:"w-full h-full bg-gradient-to-br from-slate-50 to-slate-100 p-6 overflow-auto",children:f.jsxs("div",{className:"max-w-7xl mx-auto",children:[f.jsxs("div",{className:"mb-6",children:[f.jsxs("h1",{className:"text-3xl font-bold text-gray-800 mb-2 flex items-center",children:[f.jsx(r,{className:"mr-3 text-yellow-500"}),"RLHF/PPO Training Pipeline with Performance Profiling"]}),f.jsx("p",{className:"text-gray-600",children:"Proximal Policy Optimization for Reinforcement Learning from Human Feedback"})]}),f.jsxs("div",{className:"grid grid-cols-1 lg:grid-cols-3 gap-6 mb-6",children:[f.jsxs("div",{className:"lg:col-span-2 space-y-6",children:[f.jsxs("div",{className:"bg-white rounded-lg shadow-md p-6",children:[f.jsxs("h2",{className:"text-xl font-bold mb-4 flex items-center",children:[f.jsx(a,{className:"mr-2"}),"Optimization Strategy"]}),f.jsxs("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-4",children:[f.jsx(j,{level:"none",title:"Baseline",speedup:1,features:["No optimizations","Sequential processing","High GIL contention","Inefficient memory"]}),f.jsx(j,{level:"basic",title:"Basic Optimization",speedup:1.43,features:["Vectorized operations","NumPy broadcasting","Reduced Python loops","Memory pooling"]}),f.jsx(j,{level:"advanced",title:"Advanced",speedup:2.5,features:["GPU acceleration","Mixed precision (FP16)","Gradient accumulation","Parallel data loading"]}),f.jsx(j,{level:"optimal",title:"Optimal",speedup:4,features:["Flash Attention","Tensor parallelism","Compiled kernels","Zero-copy transfers"]})]})]}),f.jsxs("div",{className:"bg-white rounded-lg shadow-md p-6",children:[f.jsx("h2",{className:"text-xl font-bold mb-4",children:"Training Control"}),f.jsxs("div",{className:"flex gap-3 mb-4",children:[f.jsxs("button",{onClick:()=>{s(!0),m(0),x(null),v([]),y("Starting RLHF/PPO training pipeline","success"),y(`Optimization level: ${p||"none"}`,"info"),y("Batch size: 8, Sequence length: 128","info");let e=0;const n=[],t=setInterval(()=>{if(e>=20){clearInterval(t),s(!1);const e=n.reduce((e,s)=>e+s.computeTime,0)/n.length,r=n.reduce((e,s)=>e+s.gpuUtil,0)/n.length,a=n.reduce((e,s)=>e+s.cpuUtil,0)/n.length,l=n.reduce((e,s)=>e+s.throughput,0)/n.length,i=n.reduce((e,s)=>e+s.gilContention,0)/n.length;return y(`Training completed! Average throughput: ${l.toFixed(1)} samples/sec`,"success"),void x({steps:n,summary:{avgComputeTime:e.toFixed(0),avgGpuUtil:r.toFixed(1),avgCpuUtil:a.toFixed(1),avgThroughput:l.toFixed(1),avgGil:i.toFixed(1)}})}const r=((e,s)=>{performance.now();let n=1e3,t=45,r=65,a=2048,l=8;switch(s){case"basic":n=700,t=65,r=55,l=11;break;case"advanced":n=400,t=85,r=40,a=1536,l=20;break;case"optimal":n=250,t=92,r=35,a=1024,l=32}const i=.5+.3*Math.random()-.15+.01*e,o=2.5-.05*e+.2*Math.random(),d=1.8-.04*e+.15*Math.random(),c=.02+.01*Math.random();return performance.now(),{step:e,computeTime:n+(100*Math.random()-50),gpuUtil:t+(10*Math.random()-5),cpuUtil:r+(10*Math.random()-5),memoryMB:a,throughput:l+(2*Math.random()-1),avgReward:Math.max(0,Math.min(1,i)),policyLoss:Math.max(.1,o),valueLoss:Math.max(.1,d),klDiv:Math.max(0,c),gilContention:"none"===s?25+10*Math.random():"basic"===s?15+5*Math.random():"advanced"===s?8+3*Math.random():3+2*Math.random()}})(e,p);n.push(r),m(e),e%5==0&&y(`Step ${e}: Reward=${r.avgReward.toFixed(3)}, Loss=${r.policyLoss.toFixed(3)}, GPU=${r.gpuUtil.toFixed(0)}%`,"info"),e++},400);b.current=t},disabled:e,className:"flex items-center px-4 py-2 bg-green-500 text-white rounded-lg hover:bg-green-600 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors",children:[f.jsx(l,{className:"mr-2",size:18}),"Start Training"]}),f.jsxs("button",{onClick:_,disabled:!e,className:"flex items-center px-4 py-2 bg-red-500 text-white rounded-lg hover:bg-red-600 disabled:bg-gray-400 disabled:cursor-not-allowed transition-colors",children:[f.jsx(i,{className:"mr-2",size:18}),"Stop"]}),f.jsxs("button",{onClick:()=>{_(),m(0),x(null),v([]),y("Training reset","info")},className:"flex items-center px-4 py-2 bg-blue-500 text-white rounded-lg hover:bg-blue-600 transition-colors",children:[f.jsx(o,{className:"mr-2",size:18}),"Reset"]})]}),e&&f.jsxs("div",{className:"mb-4",children:[f.jsxs("div",{className:"flex justify-between text-sm mb-1",children:[f.jsx("span",{children:"Training Progress"}),f.jsxs("span",{children:[n,"/20 steps"]})]}),f.jsx("div",{className:"w-full bg-gray-200 rounded-full h-3",children:f.jsx("div",{className:"bg-gradient-to-r from-blue-500 to-purple-500 h-3 rounded-full transition-all duration-300",style:{width:n/20*100+"%"}})})]}),g&&g.steps.length>0&&f.jsxs("div",{className:"grid grid-cols-2 md:grid-cols-4 gap-4",children:[f.jsxs("div",{className:"bg-blue-50 p-3 rounded-lg",children:[f.jsx("div",{className:"text-xs text-gray-600 mb-1",children:"Avg GPU Util"}),f.jsxs("div",{className:"text-2xl font-bold text-blue-600",children:[g.summary.avgGpuUtil,"%"]})]}),f.jsxs("div",{className:"bg-green-50 p-3 rounded-lg",children:[f.jsx("div",{className:"text-xs text-gray-600 mb-1",children:"Throughput"}),f.jsxs("div",{className:"text-2xl font-bold text-green-600",children:[g.summary.avgThroughput," ",f.jsx("span",{className:"text-sm",children:"samp/s"})]})]}),f.jsxs("div",{className:"bg-purple-50 p-3 rounded-lg",children:[f.jsx("div",{className:"text-xs text-gray-600 mb-1",children:"Compute Time"}),f.jsxs("div",{className:"text-2xl font-bold text-purple-600",children:[g.summary.avgComputeTime," ",f.jsx("span",{className:"text-sm",children:"ms"})]})]}),f.jsxs("div",{className:"bg-yellow-50 p-3 rounded-lg",children:[f.jsx("div",{className:"text-xs text-gray-600 mb-1",children:"GIL Contention"}),f.jsxs("div",{className:"text-2xl font-bold text-yellow-600",children:[g.summary.avgGil,"%"]})]})]})]}),g&&f.jsxs("div",{className:"bg-white rounded-lg shadow-md p-6",children:[f.jsxs("h2",{className:"text-xl font-bold mb-4 flex items-center",children:[f.jsx(d,{className:"mr-2"}),"Performance Analysis"]}),f.jsxs("div",{className:"mb-6",children:[f.jsxs("h3",{className:"font-bold text-lg mb-3 text-green-600",children:["Speedup: ","none"===p?1:"basic"===p?1.43:"advanced"===p?2.5:"optimal"===p?4:1,"x faster than baseline"]}),f.jsxs("div",{className:"space-y-3",children:[f.jsxs("div",{children:[f.jsxs("div",{className:"flex justify-between text-sm mb-1",children:[f.jsx("span",{children:"GPU Utilization"}),f.jsxs("span",{children:[g.summary.avgGpuUtil,"%"]})]}),f.jsx("div",{className:"w-full bg-gray-200 rounded-full h-2",children:f.jsx("div",{className:"bg-blue-500 h-2 rounded-full",style:{width:`${g.summary.avgGpuUtil}%`}})})]}),f.jsxs("div",{children:[f.jsxs("div",{className:"flex justify-between text-sm mb-1",children:[f.jsx("span",{children:"CPU Utilization"}),f.jsxs("span",{children:[g.summary.avgCpuUtil,"%"]})]}),f.jsx("div",{className:"w-full bg-gray-200 rounded-full h-2",children:f.jsx("div",{className:"bg-green-500 h-2 rounded-full",style:{width:`${g.summary.avgCpuUtil}%`}})})]}),f.jsxs("div",{children:[f.jsxs("div",{className:"flex justify-between text-sm mb-1",children:[f.jsx("span",{children:"GIL Contention (lower is better)"}),f.jsxs("span",{children:[g.summary.avgGil,"%"]})]}),f.jsx("div",{className:"w-full bg-gray-200 rounded-full h-2",children:f.jsx("div",{className:"bg-red-500 h-2 rounded-full",style:{width:`${g.summary.avgGil}%`}})})]})]})]}),f.jsxs("div",{className:"bg-gradient-to-r from-blue-50 to-purple-50 p-4 rounded-lg border border-blue-200",children:[f.jsxs("h4",{className:"font-bold mb-2 flex items-center",children:[f.jsx(c,{className:"mr-2 text-blue-600",size:18}),"Key Optimizations Applied"]}),f.jsxs("ul",{className:"text-sm space-y-1",children:["basic"===p&&f.jsxs(f.Fragment,{children:[f.jsx("li",{children:"âœ“ Vectorized advantage computation"}),f.jsx("li",{children:"âœ“ In-place tensor operations"}),f.jsx("li",{children:"âœ“ Reduced Python overhead"})]}),"advanced"===p&&f.jsxs(f.Fragment,{children:[f.jsx("li",{children:"âœ“ GPU-accelerated policy forward passes"}),f.jsx("li",{children:"âœ“ Mixed precision training (FP16)"}),f.jsx("li",{children:"âœ“ Asynchronous data loading"}),f.jsx("li",{children:"âœ“ Gradient accumulation for larger batches"})]}),"optimal"===p&&f.jsxs(f.Fragment,{children:[f.jsx("li",{children:"âœ“ Flash Attention for memory efficiency"}),f.jsx("li",{children:"âœ“ Compiled CUDA kernels for PPO updates"}),f.jsx("li",{children:"âœ“ Model parallelism across GPUs"}),f.jsx("li",{children:"âœ“ Zero-copy memory transfers"}),f.jsx("li",{children:"âœ“ Custom fused operations"})]}),"none"===p&&f.jsx("li",{className:"text-gray-600",children:"No optimizations applied (baseline)"})]})]})]})]}),f.jsxs("div",{className:"bg-white rounded-lg shadow-md p-6",children:[f.jsx("h2",{className:"text-xl font-bold mb-4",children:"Training Logs"}),f.jsx("div",{className:"bg-gray-900 text-gray-100 p-4 rounded-lg h-96 overflow-y-auto font-mono text-xs",children:0===h.length?f.jsx("div",{className:"text-gray-500",children:"No logs yet. Start training to see output."}):h.map((e,s)=>f.jsxs("div",{className:"mb-1",children:[f.jsxs("span",{className:"text-gray-500",children:["[",e.time,"]"]})," ",f.jsx("span",{className:"success"===e.type?"text-green-400":"warning"===e.type?"text-yellow-400":"error"===e.type?"text-red-400":"text-gray-300",children:e.message})]},s))})]})]}),f.jsxs("div",{className:"bg-white rounded-lg shadow-md p-6",children:[f.jsx("h2",{className:"text-xl font-bold mb-4",children:"Python Implementation Reference"}),f.jsx("pre",{className:"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto text-sm",children:'# Mini RLHF/PPO Training Pipeline with Profiling\n\nimport torch\nimport torch.nn as nn\nimport time\nimport psutil\nimport threading\nfrom dataclasses import dataclass\nfrom typing import Dict, List\n\n@dataclass\nclass PPOConfig:\n    batch_size: int = 8\n    seq_length: int = 128\n    gamma: float = 0.99\n    gae_lambda: float = 0.95\n    clip_epsilon: float = 0.2\n    value_coef: float = 0.5\n    entropy_coef: float = 0.01\n\nclass Profiler:\n    def __init__(self):\n        self.metrics = []\n        self.start_time = None\n        \n    def start(self):\n        self.start_time = time.perf_counter()\n        \n    def record(self, step: int, **kwargs):\n        elapsed = time.perf_counter() - self.start_time\n        cpu_percent = psutil.cpu_percent(interval=0.1)\n        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024\n        \n        self.metrics.append({\n            \'step\': step,\n            \'time\': elapsed,\n            \'cpu_percent\': cpu_percent,\n            \'memory_mb\': memory_mb,\n            **kwargs\n        })\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, vocab_size, hidden_size, num_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(hidden_size, 8, 1024),\n            num_layers\n        )\n        self.policy_head = nn.Linear(hidden_size, vocab_size)\n        self.value_head = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.transformer(x)\n        logits = self.policy_head(x)\n        values = self.value_head(x)\n        return logits, values\n\ndef compute_gae(rewards, values, next_values, dones, gamma=0.99, lam=0.95):\n    """Generalized Advantage Estimation - OPTIMIZED"""\n    advantages = torch.zeros_like(rewards)\n    last_gae = 0\n    \n    # Vectorized computation (advanced optimization)\n    for t in reversed(range(len(rewards))):\n        delta = rewards[t] + gamma * next_values[t] * (1 - dones[t]) - values[t]\n        last_gae = delta + gamma * lam * (1 - dones[t]) * last_gae\n        advantages[t] = last_gae\n        \n    return advantages\n\ndef ppo_update(policy, old_logprobs, states, actions, advantages, returns, \n               clip_epsilon=0.2, value_coef=0.5, entropy_coef=0.01):\n    """PPO clipped objective with value loss"""\n    logits, values = policy(states)\n    \n    # Policy loss with clipping\n    log_probs = torch.log_softmax(logits, dim=-1)\n    action_log_probs = log_probs.gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n    ratio = torch.exp(action_log_probs - old_logprobs)\n    \n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n    policy_loss = -torch.min(surr1, surr2).mean()\n    \n    # Value loss\n    value_loss = ((values.squeeze(-1) - returns) ** 2).mean()\n    \n    # Entropy bonus for exploration\n    entropy = -(log_probs * torch.exp(log_probs)).sum(-1).mean()\n    \n    total_loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n    \n    return total_loss, policy_loss.item(), value_loss.item(), entropy.item()\n\n# Training loop with profiling\ndef train_rlhf_ppo(config: PPOConfig, num_steps=100, device=\'cuda\'):\n    profiler = Profiler()\n    profiler.start()\n    \n    policy = PolicyNetwork(1000, 256, 4).to(device)\n    optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)\n    \n    for step in range(num_steps):\n        # Generate rollouts\n        states = torch.randint(0, 1000, (config.batch_size, config.seq_length)).to(device)\n        \n        with torch.no_grad():\n            logits, values = policy(states)\n            actions = torch.multinomial(torch.softmax(logits, dim=-1), 1).squeeze(-1)\n        \n        # Simulate rewards from reward model\n        rewards = torch.randn(config.batch_size, config.seq_length).to(device)\n        dones = torch.zeros_like(rewards)\n        \n        # Compute advantages\n        next_values = torch.cat([values[:, 1:], torch.zeros_like(values[:, :1])], dim=1)\n        advantages = compute_gae(rewards, values.squeeze(-1), next_values.squeeze(-1), dones)\n        returns = advantages + values.squeeze(-1)\n        \n        # PPO update\n        old_log_probs = torch.log_softmax(logits, dim=-1).gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n        total_loss, policy_loss, value_loss, entropy = ppo_update(\n            policy, old_log_probs.detach(), states, actions, advantages, returns\n        )\n        \n        optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n        optimizer.step()\n        \n        # Record metrics\n        profiler.record(\n            step, \n            policy_loss=policy_loss,\n            value_loss=value_loss,\n            avg_reward=rewards.mean().item()\n        )\n        \n        if step % 10 == 0:\n            print(f"Step {step}: Loss={total_loss.item():.4f}, Reward={rewards.mean():.4f}")\n    \n    return profiler.metrics\n\nif __name__ == "__main__":\n    config = PPOConfig()\n    metrics = train_rlhf_ppo(config, num_steps=100)\n    print(f"Training completed! Avg time per step: {sum(m[\'time\'] for m in metrics)/len(metrics):.3f}s")'})]})]})})};function _(){return f.jsx("div",{className:"App",children:f.jsx(y,{})})}n(b).createRoot(document.getElementById("root")).render(f.jsx(m.StrictMode,{children:f.jsx(_,{})}));
